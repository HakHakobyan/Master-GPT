---
title: "Data Mining Homework"
author: "Hakob Hakobyan"
date: "2023-03-16"
output:
  html_document:
    toc: true
    toc_float: true
---
## Task 1: Data Cleaning

### importing libraries

```{r message=FALSE, warning=FALSE}
library(haven)
library(tidyverse)
library(dplyr)
library(visdat)
library(gridExtra)
library(kableExtra)
library(MatchIt)
library(cowplot)
library(tableone)
library(ggplot2)
library(ggcorrplot)
```

### Loading the data...

```{r}
data <- read.csv("HW1.csv")
glimpse(data)
```

### Detecting inconsistencies and different types of missing values.

```{r}
sum(duplicated(data$ID))
sapply(data %>% select(-ID, -Pre_Date, -Post_Date, -P2, -Pre_Pre, -income, -spendings), table)
colSums(is.na(data))
```

Now we can draw the following insights:

* P2 has 2 NAs
* Pre_P3 has missing values [98, 99, 100], and Post_P3: [99, 100]
* Pre_P4 has missing values [98, 99, 100], and Post_P4: [99, 100]
* Pre_P5 has missing values [99, 100], and Post_P5: [99]
* Pre_P6 has inconsistencies related to data entry errors [1w, 2w], also missing values [99, 100]
* Post_P6 has missing values [99, 100] and 1 NA
* Pre_P7 has missing values [99, 100]
* Post_P7 has missing values [99]
* Pre_P9 has missing values [99, 100], and Post_P9: [98, 99]
* There is inconsistency in Pre_Answers, probably data entry errors, as we've got [-5, -6] knowing that the number of correct answers can't be negative

1. We don't have any duplicate IDs in our data
2. We have got Pre_P1 and Post_P1 which perfectly match each other
3. It looks like we don't have any issues related to NAs or inconsistency in the rest of the columns

So we've got 2 types of **missing values** that are defined in the survey: [98, 99], randomly missing values (NAs) and 100s, which can be data entry errors and can be replaced in some cases.

Some of the columns containing the value "100" may violate **membership constraints**, as in some cases, for example for P3,  there is a specific set of values that the variable can take and 100 is not part of the expected set of values for those columns.

In other cases, it may be illogical for a certain column to have a value of 100, such as the number of people living in a house, and that would be an **inconsistency** because it is outside the expected range of values for those variables.

In the case of the P3 variable we can convert "100" into the mean of math grade considering that "100" is a value error, so as not to lose information, but in the other membership constraint cases where we don't have numeric but categorical variables, it won't be relevant to replace the missing values by the mean.

### Converting missing values to "NA"s

```{r}
data$Pre_P3<- ifelse(data$Pre_P3==98 | data$Pre_P3==99, NA, data$Pre_P3)
data$Post_P3<- ifelse(data$Post_P3==98 | data$Post_P3==99, NA, data$Post_P3)
data$Pre_P4<- ifelse(data$Pre_P4>3, NA, data$Pre_P4)
data$Post_P4<- ifelse(data$Post_P4>3, NA, data$Post_P4)
data$Pre_P5<- ifelse(data$Pre_P5>3, NA, data$Pre_P5)
data$Post_P5<- ifelse(data$Post_P5>3, NA, data$Post_P5)
data$Pre_P6<- ifelse(data$Pre_P6==99 | data$Pre_P6==100, NA, data$Pre_P6)
data$Post_P6<- ifelse(data$Post_P6==99 | data$Post_P6==100, NA, data$Post_P6)
data$Pre_P7<- ifelse(data$Pre_P7==99, NA, data$Pre_P7)
data$Post_P7<- ifelse(data$Post_P7==99, NA, data$Post_P7)
data$Pre_P9<- ifelse(data$Pre_P9==99, NA, data$Pre_P9)
data$Post_P9<- ifelse(data$Post_P9 == 98 | data$Post_P9 == 99, NA, data$Post_P9)
```

### How many missing values are there in the data?

```{r}
sum(colSums(is.na(data)))
colSums(is.na(data))
vis_miss(data)
```

### FIxing the inconsistent values of two variables

```{r}
sapply(data %>% select(Pre_Answers, Pre_P3, Post_P3, Pre_P9, Pre_P7, Pre_P6), table)
data$Pre_Answers<- abs(data$Pre_Answers)
data$Pre_P3<- ifelse(data$Pre_P3==100, round(mean(filter(data, Pre_P3 != 100)$Pre_P3), 0), data$Pre_P3)
data$Post_P3<- ifelse(data$Post_P3==100, round(mean(filter(data, Post_P3 != 100)$Post_P3), 0), data$Post_P3)
data$Pre_P6<- gsub("w", "", data$Pre_P6)
data$Pre_P7<- ifelse(data$Pre_P7==100, round(mean(filter(data, Pre_P7 != 100)$Pre_P7), 0), data$Pre_P7)
data$Pre_P9<- ifelse(data$Pre_P9==100, round(mean(filter(data, Pre_P9 != 100)$Pre_P9), 0), data$Pre_P9)
```

### Fixing the datatypes

```{r}
sapply(data, class)
data$ID<-as.character(data$ID)
data$Pre_P1<-as.character(data$Pre_P1)
data$Post_P1<-as.character(data$Post_P1)
data$P2<-as.Date(data$P2, "%m/%d/%Y")
data$Pre_P4<-as.factor(data$Pre_P4)
data$Post_P4<-as.factor(data$Post_P4)
data$Pre_P5<-as.factor(data$Pre_P5)
data$Post_P5<-as.factor(data$Post_P5)
data$Pre_P6<-as.factor(data$Pre_P6)
data$Post_P6<-as.factor(data$Post_P6)
data$Pre_Date<-as.Date(data$Pre_Date, "%m/%d/%Y")
data$Post_Date<-as.Date(data$Post_Date, "%m/%d/%Y")
sapply(data, class)
```
### Creating two variables of age using dates

```{r}
data <- data %>% mutate(Pre_age = as.numeric(round((Pre_Date-P2)/365.25, 0)),  Post_age = as.numeric(round((Post_Date-P2)/365.25, 0)))
```

### Dropping the unnecessery columns

```{r}
data<- data %>% select(-Pre_P1, -Pre_P6, -Post_P6, -Pre_Pre)
data<- na.omit(data)
```

### Adding a new feature: computers per person
As I assume this variable will represent the balance issue in P9 and P7 variables better, I will create the cpp relative variable which shows how many computers each person in the family has.

```{r}
data<- data %>%
  mutate(Pre_cpp = Pre_P9/Pre_P7) %>%
  rename("Pre_tty" = Pre_Treatmen.type)
```

### Showing the structure and dimension of the data and classifying the variables as qualitive or quantitive.

```{r}
str(data)
qualitives <- data %>% select(ID, Pre_tty, Pre_P4, Pre_P5, Post_P1, Post_P4, Post_P5)
quantitives <- data %>% select(-ID, -Pre_tty, -Pre_P4, -Pre_P5, -Post_P1, -Post_P4, -Post_P5)
```

## Task 2: Summary Statistics

### Function 1

```{r}
chifunction <- function(df, cat_var, treat_var){

  table <- df %>% 
    group_by({{treat_var}}, {{cat_var}}) %>% 
    summarise(n = n()) %>%
    mutate(perc = round(n / sum(n) * 100, 2)) %>% 
    rename("Treatment Type" = {{treat_var}},
           "Number of observations" = n,
           "Percentage" = perc)
  chi <- chisq.test(table(df %>% select({{cat_var}}, {{treat_var}})))
  table <- kable(table) %>%
    add_footnote(paste("The X-squared value is equal to", round(chi$statistic, 5), "and the p-value is", round(chi$p.value, 7)))
  
  return(table)
}
```

### Function 2

```{r}
anovafunction <- function(df, num_var, treat_stat){
  table <- df %>%
    group_by({{treat_stat}}) %>%
    summarise(Mean = round(mean({{num_var}}), 3),
              std = round(sd({{num_var}}), 3)) %>%
    rename("Treatment Type" = {{treat_stat}},
           "Standard deviation" = std)
  model <- lm(paste(deparse(substitute(num_var)), " ~ ", deparse(substitute(treat_stat))), data = df)
  anova_result<- anova(model)
  f_value <- round(anova_result$F[1], 5)
  p_value <- round(pf(f_value, anova_result$Df[1], anova_result$Df[2], lower.tail = FALSE), 5)
  table <- kable(table) %>% 
    add_footnote(paste("The f-value of ANOVA is equal to", f_value, "and the p-value is", p_value))
  
  return(table)
}
```

#### The mean and variability of spendings by groups

```{r}
anovafunction(data, spendings, Pre_tty)
```

It looks like we don't have any statistically significant issues here.

#### The number of observations grouped by gender and treatment type.

```{r message=FALSE, warning=FALSE}
chifunction(data, Post_P1, Pre_tty)
```

Although we have got a statistically significent p-value close to 0, which means we can reject our null hypothesis, that is the gender variable is not balanced in different treatment groups, we assume that this will not affect the final results.

#### The number of observations grouped by overall performance for the semester 1 and treatment type.

```{r message=FALSE, warning=FALSE}
chifunction(data, Pre_P4, Pre_tty)
```

We don't have anny issues with balance in Pre_P4 too.

### The relationship between age and treatment type.

```{r}
anovafunction(data, Pre_age, Pre_tty)
```

The F-value of 6.36704 suggests that there is a significant difference in the means of the age variable across the different treatment types. The p-value of 0.00248 indicates that this effect is statistically significant at a level of significance of 0.05, which means that we can reject the null hypothesis that there is no relationship between age and treatment type.

### Balancing the data

#### Changing the levels of variable Pre_Treatmen.type
```{r}
data<- data %>%
  mutate(Ttype = ifelse(Pre_tty == "C", 0, 1))
```

#### Using 3 meaningful pre-treatment characteristics of students as covariates to create Matchit object, then matched data.

For now we have got a significantly low p-values for age and gender variable. This means that these variables are not balanced in different treatment groups. We assume that the imbalance of gender won't affect the final results of treatment groups, so **at first sight** we don't consider taking it as a covariate in our Matchit object yet, but only taking the age variable based on the significance of its ANOVA value, and two more variables.

```{r warning=FALSE}
mob <- matchit(Ttype ~ Pre_age + Pre_P5 + Post_P1, data = data, method = "optimal")
mdata <- match.data(mob)
```

This actually was a very challenging task to find out how to match the data for balance.

First I have tried two matchit methods "nearest" and "optimal", and the results show that the "optimal" method works better for our data (as the "nearest" method made the balance issue even deeper).

Secondly I needed to choose which variables to choose as covariates to create Matchit object. On the first sight it seems quite simple to select those variables which have statistically significant or relatively low p-values like Pre_age, Pre_cpp and Pre_P5. And we wouldn't select the gender variable, because as I mentioned above it wouldn't affect the final results.

**BUTTTT**

The results showed that when we try to balance any variables in the different treatment groups, it turns out we break the balance for other variables. So what was left to do was to try different combinations of variables as covariates in Matchit object, and to choose the best trade-off between them (which was a really thankless thing to do).

That's why I have selected Pre_age, Pre_P5, Post_P1 in the matchit function to balance the data. We will compare the before and after p-values for different variables below (**SPOILER**: you will see that p-values for almost all the considered variables have improved).

```{r message=FALSE, warning=FALSE}
anovafunction(data, Pre_age, Pre_tty)
anovafunction(mdata, Pre_age, Pre_tty)
chifunction(data, Pre_P5, Pre_tty)
chifunction(mdata, Pre_P5, Pre_tty)
anovafunction(data, Pre_cpp, Pre_tty)
anovafunction(mdata, Pre_cpp, Pre_tty)
chifunction(data, Pre_P4, Pre_tty)
chifunction(mdata, Pre_P4, Pre_tty)
anovafunction(data, Pre_P3, Pre_tty)
anovafunction(mdata, Pre_P3, Pre_tty)
chifunction(data, Post_P1, Pre_tty)
chifunction(mdata, Post_P1, Pre_tty)
```


So here we got a new matched data, which looks like more balanced than the initial one. Here we'll visually see the change of the balance for the most problematic variables below.

```{r}
par(mfrow = c(2, 2))
boxplot(Pre_age ~ Pre_tty, data, horizontal = TRUE)
boxplot(Pre_cpp ~ Pre_tty, data, horizontal = TRUE)
boxplot(Pre_age ~ Pre_tty, mdata, horizontal = TRUE)
boxplot(Pre_cpp ~ Pre_tty, mdata, horizontal = TRUE)
par(mfrow = c(1, 1))
```

```{r}
plot1<- ggplot(data, aes(x = Pre_tty, color = Pre_P5))+
  geom_bar(position = "dodge")+
  ggtitle("Initial data")
plot2<- ggplot(mdata, aes(x = Pre_tty, color = Pre_P5))+
  geom_bar(position = "dodge")+
  ggtitle("Matched data")
grid.arrange(plot1, plot2)
```

#### Comparing p-value results of data before and after matching.

```{r message=FALSE, warning=FALSE}
unmatched_tbl <- CreateTableOne(vars = c("Pre_age", "Pre_cpp", "Pre_P5"), strata = "Pre_tty", data = data)
print(unmatched_tbl)
matched_tbl <- CreateTableOne(vars = c("Pre_age", "Pre_cpp", "Pre_P5"), strata = "Pre_tty", data = mdata)
print(matched_tbl)
```

## Task 3: Data Visualization

### Describe one numeric variable and the variable treatment type (separately). Comment on it.

```{r}
summary(mdata$Pre_P3)
ggplot(mdata, aes(x = Pre_P3))+
  geom_bar()
```

Here we see the descriptive statistics for math grade of the students. Most of the values are concentrated around 8 as mean is about 8, and the mode is 9. This means most of the observed students are good (at least not bad) at maths.

```{r}
table(mdata$Pre_tty)
ggplot(mdata, aes(x = Pre_tty))+
  geom_bar()
```

Here we have C group bigger than each of the two treatment groups, and the two treatment groups have almost equal number of observations.

### By using the dataset reproduce the following graph:

```{r}
afunction <- function(df, cat_var, treat_var) {
  table <- df %>%
    group_by({{treat_var}}, {{cat_var}}) %>%
    summarise(n = n()) %>%
    mutate(perc = round(n / sum(n) * 100, 2)) %>%
    rename("Treatment Type" = {{treat_var}},
           "Number of observations" = n,
           "Percentage" = perc)
  return(table)
}
```

```{r message=FALSE, warning=FALSE}
data_summary <- afunction(data, Pre_P4, Pre_tty)
data_summary_post <- afunction(data, Post_P4, Pre_tty)

base_plot <- ggplot() +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5))

plot3 <- base_plot +
  geom_bar(data = data, aes(x = Pre_tty, fill = Pre_P4), position = "dodge") +
  labs(x = NULL, y = "Count") +
  scale_fill_manual(values = c("1", "2", "3"), labels = c("Excellent", "Satisfactory", "Fair")) +
  geom_text(data = data_summary, aes(x = `Treatment Type`, y = `Number of observations`, label = `Number of observations`, group = Pre_P4), position = position_dodge(width = 0.9), vjust = -0.5) +
  geom_text(data = data_summary, aes(x = `Treatment Type`, y = `Number of observations`, label = paste0(Percentage, "%"), group = Pre_P4), position = position_dodge(width = 0.9), vjust = -1.5)+
  theme(legend.position = "none")

plot4 <- base_plot +
  geom_bar(data = data, aes(x = Pre_tty, fill = Post_P4), position = "dodge") +
  labs(x = NULL, y = NULL, fill = NULL) +
  scale_fill_manual(values = c("1", "2", "3"), labels = c("Excellent", "Satisfactory", "Fair")) +
  geom_text(data = data_summary_post, aes(x = `Treatment Type`, y = `Number of observations`, label = `Number of observations`, group = Post_P4), position = position_dodge(width = 0.9), vjust = -0.5) +
  geom_text(data = data_summary_post, aes(x = `Treatment Type`, y = `Number of observations`, label = paste0(Percentage, "%"), group = Post_P4), position = position_dodge(width = 0.9), vjust = -1.5)+
  theme(legend.position = "none")

combined_plot <- plot_grid(NULL,
  get_legend(plot4 + theme(legend.position="top")),
  plot_grid(plot3, plot4, ncol = 2),
  nrow = 3,
  rel_heights = c(0.1, 0.1, 2)
)

print(combined_plot + ggtitle("Treatment type by overall performance (Pre, Post)") + theme(plot.title = element_text(hjust = 0.5)))
```

### Consider grouped graphical comparisons for the chosen numeric variable and treatment type. Comment on it.

```{r}
ggplot(data, aes(x = Pre_tty, y = Pre_P3, fill = Pre_tty)) +
  geom_boxplot() +
  labs(x = "Treatment Type", y = "Math grade") +
  ggtitle("Grouped comparison of pre math grade by treatment type")
```

As we can see in the graph the means of the math grades in different treatment groups are the same, and almost so are the Q1 and Q3 values, which means that there is not a balance issue related to this variable in our dataset. From this graph we can also see that T1 has outliers from a wider range than the other groups.

### Make meaningful conclusions based on b and c (remember about the randomization).

*Based on the b chart we can conclude that the different Pre_P4 values are distributed in the different treatment groups almost equally, or at least there is not statistically significant differences among the distribution of different P4 values in different groups.

*Comparing the Pre and Post P4 values we see that after treatment the math grade of the students have been improved in both control and treatment groups, in particular in the groups C and T1 more changes were from "Fair" to "Satisfactory", while in the T2 group more students turned from "Satisfactory" to Excellent" level.

*As the levels of P4 variable are distributed equally across different treatment groups in our dataset, and the means of P3 are equal, this is an indication that randomization has been successful to some extent (at leased for P3 and P4). However, it does not guarantee that randomization has been done perfectly as previously in our research we have observed variables that had issues with balance.

### Find the pattern between savings and income, use three variables (third must be categorical). Comment on it.

```{r}
data<- data %>%
  mutate(savings = income - spendings)

ggplot(data, aes(x = income, y = savings, color = Pre_tty)) +
  geom_point() +
  facet_wrap(~ Pre_tty) +
  geom_smooth(method = "lm", se = FALSE)+
  theme_minimal()
```

In this plot we can see that there is a negative linear dependence between income and savings variables in C group, but when viewing in a large scale (it is shown in the following charts) the line turns out almost parallel to the x axis, which shows the very low correlation coefficient between these variables. This graphs also show that in T groups the relationship looks like positive and in C group it is negative, but these relationships can't be considered as statistically significant.

### (Confirm your findings) Compute the correlation between the variables, visualize correlation by using the package ggcorrplot. Does any type of relationship exist among variables if the Pearson correlation coefficient is near 0?

```{r message=FALSE, warning=FALSE}
quantitives<- quantitives %>%
  mutate(savings = income - spendings) %>%
  select(-Pre_Date, -Post_Date, -P2)

corr_matrix <- cor(quantitives)
ggcorrplot(corr_matrix, type = "lower", lab = TRUE)

cor(data$savings, data$income)
cor(data$spendings, data$income)
cor(data$savings, data$spendings)
```

As we know savings = income - spending. So we know that there is connection between these variables. So even though based on the data there is no linear dependence between income and savings as its correlation is almost equal to 0, it is not reasonable to claim that there is no connection between them. The correlation between income and savings is equal to 0.49, and between savings and spendings: (-0.88).

```{r message=FALSE, warning=FALSE}
ggplot(data, aes(x = income, y = savings, color = Pre_tty)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, color = "black") +
  labs(title = "Scatterplot of income and savings with smoothed curve")+
  theme_bw() +
  xlim(375, 425) +
  ylim(120, 180)
```

After thoroughly analyzing our dataset using various charts, we found that no clear relationship exists between income and savings. Due to the limited size of our dataset (105 observations) and the high variance between the observations, we cannot conclusively confirm or reject the presence of any relationship between these variables. Further research with a larger dataset and additional variables may be necessary to better understand the potential relationship between income and savings.

## Questions

### What is the difference between ratio and interval variables?
With interval variables, it is meaningful to perform arithmetic operations such as addition and subtraction, but multiplication and division are not meaningful since there is no true zero point in contrast to ratio variables where we can perform multiplication and division.

### Can we rely only on summary statistics in analyzing the data? Why?
No, because in some cases visualization may better represent the nature of our data.

### Can we use the bar plot to describe the numeric variable? Why? Which kind of visualization can be used for numeric data? What about the relationship between 2 numeric variables?
In some cases we have to use histograms when referring to numeric variables (for example when we have continuous numeric data and we want to see the distribution), but in discrete cases it is possible to use bar plots to describe a numeric variable(I have used a bar plot for a numeric variable once in this project when describing math grade of students). For numeric data can also be used different types of box plots, or scatter plots. In the case of having 2 numeric variables one of the best ways to represent their connection is doing it by scatter plots.

### How much time did it take you to accomplish the HW?
I started it in March 16 and ended in March 30. So it took me about 10 days to accomplish the HW (leaving out the weekends), with about 3-4 hours a day on average.